# -*- coding: utf-8 -*-
"""Untitled0.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1UYfCMzskNXvS9ppy4LkPydOhQREZQfhL
"""
import warnings

import pandas as pd
from statsmodels.stats.outliers_influence import variance_inflation_factor as vif
from sklearn.decomposition import PCA
from sklearn.preprocessing import RobustScaler
from sklearn.preprocessing import MinMaxScaler
from sklearn.model_selection import train_test_split
import tensorflow as tf
from keras import Sequential
from keras.layers import Dense
from keras.layers import LeakyReLU
import numpy as np
from sklearn.preprocessing import LabelEncoder
from sklearn.preprocessing import OneHotEncoder
import seaborn as sn
import matplotlib.pyplot as plt
import keras
pd.set_option('display.max_rows', 10000)
warnings.filterwarnings("ignore")



df = pd.read_csv(r"C:\Users\Louie\PycharmProjects\Chevron-Datathon-Project\Added_Data_Training.csv", thousands=',')
test_data = pd.read_csv(r"C:\Users\Louie\PycharmProjects\Chevron-Datathon-Project\added_testing_data2.csv",thousands=',')
corr = df.drop(columns = ["StateCode","State","TotalNumberofInvestments","TotalAmountofAssistance","Year"])

# corr_pairs = corr.corr()
# names = corr_pairs.columns.values
# upper = corr_pairs.where(np.triu(np.ones(corr_pairs.shape), k=1).astype(np.bool))

# #Find features with correlation greater than 0.95
# to_drop = [column for column in upper.columns if any(upper[column] > 0.7)]
# print(to_drop)
# # Drop features 
# df.drop(to_drop, axis=1,inplace=True)
# test_data.drop(to_drop,axis = 1,inplace=True)

def correlation(dataset,test, threshold):
    col_corr = set() # Set of all the names of deleted columns
    corr_matrix = dataset.corr()
    for i in range(len(corr_matrix.columns)):
        for j in range(i):
            if (corr_matrix.iloc[i, j] >= threshold) and (corr_matrix.columns[j] not in col_corr):
                colname = corr_matrix.columns[i] # getting the name of column
                col_corr.add(colname)
                if colname in dataset.columns:
                    del dataset[colname]
                if colname in test.columns:
                  del test[colname] # deleting the column from the dataset
correlation(df,test_data, 0.7)


one_hot = pd.get_dummies(df["State"])
df = df.drop('State',axis = 1)
df = df.drop('Year',axis = 1)
df = df.join(one_hot)

one_hot = pd.get_dummies(test_data["State"])
test_data = test_data.drop('State',axis = 1)
test_data = test_data.drop('Year',axis = 1)
test_data = test_data.join(one_hot)

X_train = df.drop(columns = ["TotalAmountofAssistance","StateCode"])
y_train = df['TotalAmountofAssistance']
X_test = test_data.drop(columns = ["TotalAmountofAssistance","StateCode"])
y_test = test_data['TotalAmountofAssistance']

X_train = np.asarray(X_train).astype('float32')
X_test = np.asarray(X_test).astype('float32')
y_train = np.asarray(y_train).astype('float32')
y_test = np.asarray(y_test).astype('float32')

# scaler = RobustScaler()
# scaler.fit(X_train)
# X_train = scaler.transform(X_train)
# X_test = scaler.transform(X_test)


def test(batchSize, numLayers, epoch):
    model = Sequential()
    model.add(Dense(batchSize, input_dim=66, activation='linear'))
    for i in range(numLayers):
        model.add(Dense(batchSize, activation='relu'))
    model.add(Dense(1,activation='linear'))
    
    optimizer = keras.optimizers.Adam(lr=0.01)
    model.compile(loss='mse', optimizer=optimizer, metrics=['mse', 'mae', tf.keras.metrics.RootMeanSquaredError()])
    
    history = model.fit(X_train, y_train, epochs=epoch, batch_size = 10, validation_split=0.2, verbose=0)
    
    history_df = pd.DataFrame(history.history)
    plt.plot(history_df['loss'], label='loss')
    #plt.plot(history_df['val_loss'], label='val_loss')
    
    plt.legend()
    
    plt.plot(history_df['val_loss'], label='val_loss')
    row = [batchSize, numLayers, epoch, model.evaluate(X_test, y_test)[3]]
    print(row)
    rows.append(row)
    
    y_pred = model.predict(X_test).flatten()
    a = plt.axes(aspect='equal')
    plt.scatter(y_test, y_pred)
    plt.xlabel('True values')
    plt.ylabel('Predicted values')
    plt.title('A plot that shows the true and predicted values')
    plt.plot()
    
    
rows = []
# batch_sizes = [10, 15, 20]
# numsLayers = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]
# epochs= [200, 250, 300, 350, 400]
batch_sizes = [15]
numsLayers = [10]
epochs= [250]

for batch_size in batch_sizes:
    for numLayers in numsLayers:
        for epoch in epochs:
            test(batch_size, numLayers, epoch)


# df = pd.DataFrame(rows)
# df.to_csv(r'C:\Users\Louie\PycharmProjects\Chevron-Datathon-Project\result.csv', encoding='gbk')
